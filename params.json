{"name":"Wells-fargo-analytics-challenge","tagline":"","body":"# Wells Fargo Analytics Challenge Overview \r\nOn November 10th, Wells Fargo unveiled it's first analytics challenge. Their main goal for the challenge was to devise an efficient way to break down different comments left on social media sites in order to determine what topics consumers were most concerned over. They gave this description to each of the competitors. \"Dialogues on social media can provide tremendous insight into the behaviors, desires, pains, and thoughts of consumers. We'd like your help in developing a repeatable process that identifies, classifies, and extracts the underlying drivers of consumer financial conversations and comments in social media data.\" \r\n\r\n## Team Plan and Goals\r\nHaving been given this request, our group devised a plan to locate and analyze different topics within the mass of data provided for the challenge. Our goals were to set up a program that would analyze the words of each individual line of text within the document and would then score each line based on the amount of positive and negative words it found. In order to do this, we needed to condense the data into a document the program could read and then recognize key words that would contribute to our analysis. Determining this, we also created the goal of utilizing our code to eliminate any useless or confusing words from our data set, allowing for cleaner and more concentrated findings. \r\n\r\n## Process \r\nOur team decided that, in order to have any insight on conversational drivers within the data, we would need to break down the data into the most frequently discussed topics, and the words that were associated with both the individual banks and the topics themselves, in order to determine which bank had the most positive responses and which bank had the most negative. To do this, we used the first several thousand lines of our refined data set to locate lines where important topics were recurring in the data, such as discussions about customer service or consumers' security. We had hoped to use this information to create a Bag of Words analysis based off of our labeling, but we were running out of time for the deadline, and decided instead that I would manually comb through the topics and find the ones that most often appeared. While this was not our original plan, manually sorting the data by positive and negative topics allowed us to not only determine what topics were most concerning to consumers, but to determine which bank was associated with a more positive feedback from customers and which had more negative feedback. We simplified the data by removing numeric text, symbols, punctuation, and useless words, and then split the data into positive and negative scores by bank. With each bank having   a list of positive and negative texts, we then used the words from each to create word clouds out of the 100 most frequent words for each bank's positive and negative data. \r\n\r\n## Analyzed Findings\r\n\r\nAfter we researched, created, and ran the code we needed for our program, we cleaned up our code and prepared a report of what we found. We looked at the percentage of positive and negative texts per bank to verify what we discovered, and the numbers corroborate our results. The percentages show that Bank B had the highest percentage of positive responses, and that Bank D had the highest percentage of negative responses. Looking back through our labels, we noted that in Bank B's  word cloud (Shown in report below) the large majority of words present corresponded to positive statements about customer service, and for Bank D's cloud (In report below) the cloud containing a large percent of negative words related to security and fraud. Checking through the topics we had found, we realized that Bank B had the most occurrences of customers satisfied or extremely pleased with the customer service they received, and Bank D had a large amount of tweets directed at a hacking issue that Bank D experienced. \r\n\r\n###Official Written Report\r\n \r\n \"220,377 posts, and two weeks to figure them all out. These are very intimidating numbers for a team of freshman with zero experience in the data science world. How does one even start something like this? The first step was to outline potential approaches. After several scrapped ideas, the final was to focus on each bank individually. To do this, we found data points containing “banka”, “bankb”, “bankc”, and “bankd”, and categorized them accordingly. Our next step was divide each bank’s posts based on their sentiment, ie. the post’s attitude, positive or negative. To find the sentiments of each data point, we used a list of positive words and a list a negative words. We compared the words in each data point to the words of the lists, and a score was recorded for each post. If a post contained a positive word, its score went up by one, and if a post contained a negative word, its score went down by one. Posts with scores above two were marked as positive, and posts with scores below negative two were marked as negative. \r\nSo, at this point, the data points have been divided into eight groups: a positive and negative group for each of the four banks. However, not every post from the original data set is a part of one of these groups. Any post that does not mention any of the banks or has a middling sentiment was removed. Also, if a post mentioned two or more of the banks, it was placed in multiple groups.\r\nNow, to determine the subject matters of the posts, we looked at each group as a large body of text. Within each of these bodies, we found the most frequently used words. This gave us some overall topics to look at for each group, instead of classifying each post individually.\r\nBelow is an outline of our thought process when approaching this project.\r\n\r\n![](https://d18qs7yq39787j.cloudfront.net/uploads/redactor_rails/picture/462/Screen_Shot_2015-11-21_at_2.36.55_PM.png)\r\n\r\n  What were these overall topics though? We found that almost every bank is either succeeding or failing in customer service, ATM's, fraud/ security, or technology/machines. We can prove this based on the data and its relationship to the original posts. By making a word cloud of both the positive and negative words in each post, we can see which words are used more. For example, the word cloud below is of bank b. \r\n\r\n![](https://d18qs7yq39787j.cloudfront.net/uploads/redactor_rails/picture/463/bankBPos.png)\r\n\r\n  This word cloud makes it clear that these are positive words and that customer service is very good in bank b. Below is a negative word cloud for bank b. Words like money, and fraud are very large and thus those are the main problems with bank b. This word cloud represents that the majority of people posting negative posts are talking about fraud/ security. \r\n\r\n![](https://d18qs7yq39787j.cloudfront.net/uploads/redactor_rails/picture/464/bankBNeg.png)\r\n\r\nThese topics we found from the words clouds were basically the same across the board. Every bank has problems in every topic, but some banks have bigger problems than others. For example, bank b has some people complaining about customer service, but they have more people saying how great the customer service is. With banks so large, and millions of customers, someone is bound to complain about each and every possible problem. By analyzing using a word cloud though, bank b can see that their largest problem is fraud and not necessarily customer service even though a few people complained about it.\"\r\n\r\n\r\nHere is the R code we used to analyze the text and generate the wordclouds.\r\n\r\n```R\r\ndf = read.table('dataset.txt',sep=\"|\",header=T)\r\ndf$FullText = as.character(df$FullText)\r\n\r\ndf.texts = as.data.frame(df[,ncol(df)])\r\n\r\ncolnames(df.texts) = 'FullText'\r\n\r\ndf.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\ndf$FullText = df.texts.clean$FullText\r\n\r\nidx.1000 = sample(1:nrow(df),1000)\r\ndf.1000 = df[idx.1000,]\r\n\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(df.texts.clean))   \r\n\r\ndocs <- tm_map(docs, stripWhitespace)\r\n\r\nbankA.idx = which( sapply( df$FullText , function( x ) grepl( \"BankA\" , x ) ) )\r\nbankB.idx = which( sapply( df$FullText , function( x ) grepl( \"BankB\" , x ) ) )\r\nbankC.idx = which( sapply( df$FullText , function( x ) grepl( \"BankC\" , x ) ) )\r\nbankD.idx = which( sapply( df$FullText , function( x ) grepl( \"BankD\" , x ) ) )\r\n\r\ndf.bankA = df[ bankA.idx ,   ]\r\ndf.bankB = df[ bankB.idx ,   ]\r\ndf.bankC = df[ bankC.idx ,   ]\r\ndf.bankD = df[ bankD.idx ,   ]\r\n\r\nfor (i in 1:nrow(df)) {\r\n  meta(docs[[i]],\"MediaType\") = df$MediaType[i]\r\n  meta(docs[[i]],\"Year\") = df$Year[i]\r\n  if (grepl(\"BankA\",df$FullText[i])) {\r\n    meta(docs[[i]],\"BankA\") = T\r\n  } else {\r\n    meta(docs[[i]],\"BankA\") = F\r\n  }  if (grepl(\"BankB\",df$FullText[i])) {\r\n    meta(docs[[i]],\"BankB\") = T\r\n  } else {\r\n    meta(docs[[i]],\"BankB\") = F\r\n  }\r\n}\r\nbankA.idx <- meta(docs, \"BankA\") == T\r\n\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx]\r\n\r\nsummary(docs)\r\n\r\ndocs <- tm_map(docs, removePunctuation) \r\n\r\n# Download and upload: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\r\n# system('unrar e opinion-lexicon-English.rar')\r\n\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n    sentence = gsub('[[:punct:]]', '', sentence)\r\n    sentence = gsub('[[:cntrl:]]', '', sentence)\r\n    sentence = gsub('\\\\d+', '', sentence)\r\n    sentence = tolower(sentence)\r\n    \r\n    word.list = str_split(sentence, '\\\\s+')\r\n    words = unlist(word.list)\r\n    \r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n  \r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n\r\nbankA.scores = score.sentiment(df.bankA$FullText, pos, neg, .progress='text')\r\nbankA.scores$very.pos = as.numeric(bankA.scores$score >= 2)\r\nbankA.scores$very.neg = as.numeric(bankA.scores$score <= -2)\r\n\r\nbankA.numpos = sum(bankA.scores$very.pos)\r\nbankA.numneg = sum(bankA.scores$very.neg)\r\n\r\nbankA.global_score = round( 100 * bankA.numpos / (bankA.numpos + bankA.numneg) )\r\n\r\nbankA.scores$mediatype = df.bankA$MediaType\r\n\r\nbankB.scores = score.sentiment(df.bankB$FullText, pos, neg, .progress='text')\r\nbankB.scores$very.pos = as.numeric(bankB.scores$score >= 2)\r\nbankB.scores$very.neg = as.numeric(bankB.scores$score <= -2)\r\n\r\nbankB.numpos = sum(bankB.scores$very.pos)\r\nbankB.numneg = sum(bankB.scores$very.neg)\r\n\r\nbankB.global_score = round( 100 * bankB.numpos / (bankB.numpos + bankB.numneg) )\r\n\r\nbankB.scores$mediatype = df.bankB$MediaType\r\n\r\nbankC.scores = score.sentiment(df.bankC$FullText, pos, neg, .progress='text')\r\nbankC.scores$very.pos = as.numeric(bankC.scores$score >= 2)\r\nbankC.scores$very.neg = as.numeric(bankC.scores$score <= -2)\r\n\r\nbankC.numpos = sum(bankC.scores$very.pos)\r\nbankC.numneg = sum(bankC.scores$very.neg)\r\n\r\nbankC.global_score = round( 100 * bankC.numpos / (bankC.numpos + bankC.numneg) )\r\n\r\nbankC.scores$mediatype = df.bankC$MediaType\r\n\r\nbankD.scores = score.sentiment(df.bankD$FullText, pos, neg, .progress='text')\r\nbankD.scores$very.pos = as.numeric(bankD.scores$score >= 2)\r\nbankD.scores$very.neg = as.numeric(bankD.scores$score <= -2)\r\n\r\nbankD.numpos = sum(bankD.scores$very.pos)\r\nbankD.numneg = sum(bankD.scores$very.neg)\r\n\r\nbankD.global_score = round( 100 * bankD.numpos / (bankD.numpos + bankD.numneg) )\r\n\r\nbankD.scores$mediatype = df.bankD$MediaType\r\n\r\nfind.freq.words <- function( bank.scores , sentiment )\r\n{\r\n  if ( sentiment == \"positive\" )\r\n  {\r\n    texts = as.data.frame( bank.scores[ bank.scores$very.pos == 1 , 2 ] )\r\n  } else {\r\n    texts = as.data.frame( bank.scores[ bank.scores$very.neg == 1 , 2 ] )\r\n  }\r\n  \r\n  \r\n  docs <- Corpus( DataframeSource( texts ) )\r\n  \r\n  docs <- tm_map( docs , removePunctuation )\r\n  docs <- tm_map( docs , removeNumbers )\r\n  docs <- tm_map( docs , tolower )\r\n  docs <- tm_map( docs , removeWords , stopwords( \"english\" ) )\r\n  docs <- tm_map( docs , stripWhitespace )\r\n  docs <- tm_map( docs , PlainTextDocument ) \r\n  \r\n  docs <- tm_map(docs, removeWords, c( \"bank\" , \"banka\" , \"bankb\" , \"bankc\" , \"bankd\" , \"name\" , \"twithndl\" , \"twithndlbanka\" , \"twithndlbankb\" , \"twithndlbankc\" , \"twithndlbankd\" , \"internet\" , \"rettwit\" ) )\r\n  \r\n  dtm  <- DocumentTermMatrix( docs )\r\n  freq <- colSums( as.matrix( dtm ) )\r\n  \r\n  return( freq )\r\n}\r\n\r\nbankA.pos.freq = find.freq.words( bankA.scores , \"positive\" )\r\nbankB.pos.freq = find.freq.words( bankB.scores , \"positive\" )\r\nbankC.pos.freq = find.freq.words( bankC.scores , \"positive\" )\r\nbankD.pos.freq = find.freq.words( bankD.scores , \"positive\" )\r\nbankA.neg.freq = find.freq.words( bankA.scores , \"negative\" )\r\nbankB.neg.freq = find.freq.words( bankB.scores , \"negative\" )\r\nbankC.neg.freq = find.freq.words( bankC.scores , \"negative\" )\r\nbankD.neg.freq = find.freq.words( bankD.scores , \"negative\" )\r\n\r\nbankA.pos.ord <- order( bankA.pos.freq )\r\nbankB.pos.ord <- order( bankB.pos.freq )\r\nbankC.pos.ord <- order( bankC.pos.freq )\r\nbankD.pos.ord <- order( bankD.pos.freq )\r\nbankA.neg.ord <- order( bankA.neg.freq )\r\nbankB.neg.ord <- order( bankB.neg.freq )\r\nbankC.neg.ord <- order( bankC.neg.freq )\r\nbankD.neg.ord <- order( bankD.neg.freq )\r\n\r\nbankA.pos.freq[ tail( bankA.pos.ord , 100 ) ]\r\nbankB.pos.freq[ tail( bankB.pos.ord , 100 ) ]\r\nbankC.pos.freq[ tail( bankC.pos.ord , 100 ) ]\r\nbankD.pos.freq[ tail( bankD.pos.ord , 100 ) ]\r\nbankA.neg.freq[ tail( bankA.neg.ord , 100 ) ]\r\nbankB.neg.freq[ tail( bankB.neg.ord , 100 ) ]\r\nbankC.neg.freq[ tail( bankC.neg.ord , 100 ) ]\r\nbankD.neg.freq[ tail( bankD.neg.ord , 100 ) ]\r\n\r\n\r\nset.seed( 142 )\r\nwordcloud( names( bankA.pos.freq ), bankA.pos.freq , min.freq = 100 )\r\nwordcloud( names( bankA.neg.freq ), bankA.neg.freq , min.freq = 100 )\r\nwordcloud( names( bankB.pos.freq ), bankB.pos.freq , min.freq = 100 )\r\nwordcloud( names( bankB.neg.freq ), bankB.neg.freq , min.freq = 100 )\r\nwordcloud( names( bankC.pos.freq ), bankC.pos.freq , min.freq = 50 )\r\nwordcloud( names( bankC.neg.freq ), bankC.neg.freq , min.freq = 50 )\r\nwordcloud( names( bankD.pos.freq ), bankD.pos.freq , min.freq = 100 )\r\nwordcloud( names( bankD.neg.freq ), bankD.neg.freq , min.freq = 100 )\r\n```\r\n\r\n## Post Project Coding\r\n\r\nAfter completing the project, I decided to try and take our word cloud a step further and use them to analyze the words present in our labeled text, as we were unable to use the labeling to create a Bag of Words. I used the Bank.A positive csv document that I first labeled, and then attempted to create code that would read the frequent words of the labeled tweets and form a word cloud. I chose to analyze the two topics that had the largest number of labels, SCY for security and CS for customer service. This would find words that were very similar and recurred commonly in each topic. Below is the code created to form the word clouds. \r\n\r\n```R \r\ndf.labels = read.csv('bankA.pos.Final.Project.list.csv')\r\nNeeded <- c(\"tm\", \"SnowballCC\", \"RColorBrewer\", \"ggplot2\", \"wordcloud\", \"biclust\", \"cluster\", \"igraph\", \"fpc\")   \r\ninstall.packages(Needed, dependencies=TRUE)   \r\n\r\ninstall.packages(\"Rcampdf\", repos = \"http://datacube.wu.ac.at/\", type = \"source\")  \r\n\r\ncname <- file.path(\"~\", \"Desktop\", \"texts\")   \r\ncname   \r\n\r\nlibrary(tm)   \r\ndf.labels.relevent = df.labels[ which(( df.labels$Label == 'SCY' ) | (df.labels$Label == 'CS')) ,   ]\r\ndocs <- Corpus(DirSource(cname))\r\n\r\ndf.labels.docs = docs[ which(( df.labels$Label == 'SCY' ) | (df.labels$Label == 'CS')) ]\r\n\r\nsummary(docs) \r\n\r\ninspect(docs[2])\r\n\r\ndocs <- tm_map(docs, removePunctuation)\r\nfor(j in seq(docs))   \r\n{   \r\n  docs[[j]] <- gsub(\"/\", \" \", docs[[j]])   \r\n  docs[[j]] <- gsub(\"@\", \" \", docs[[j]])   \r\n  docs[[j]] <- gsub(\"\\\\|\", \" \", docs[[j]])   \r\n}   \r\ndocs <- tm_map(docs, removeNumbers) \r\ndocs <- tm_map(docs, tolower) \r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))  \r\nlibrary(SnowballC)   \r\ndocs <- tm_map(docs, stemDocument) \r\n\r\nlabels.scores = score.sentiment(df.labels.relevent$FullText, pos, neg, .progress='text')\r\nlabels.scores$very.pos = as.numeric(labels.scores$score >= 2)\r\nlabels.scores$very.neg = as.numeric(labels.scores$score <= -2)\r\nlabels.numpos = sum(labels.scores$very.pos)\r\nlabels.numneg = sum(labels.scores$very.neg)\r\nlabels.global_score = round( 100 * labels.numpos / (labels.numpos + labels.numneg) )\r\nlabels.scores$mediatype = df.labels.relevent$MediaType\r\nlabels.pos.freq = find.freq.words( labels.scores , \"positive\" )\r\n\r\n\r\nlabels.pos.ord <- order( labels.pos.freq )\r\n\r\n\r\nlabels.pos.freq[ tail( labels.pos.ord , 100 ) ]\r\n\r\nlabels.texts = as.data.frame( df.labels.relevent )\r\n\r\nlabels.docs <- Corpus( DataframeSource( labels.texts ) )\r\n\r\nlabels.docs <- tm_map( labels.docs , removePunctuation )\r\nlabels.docs <- tm_map( labels.docs , removeNumbers )\r\nlabels.docs <- tm_map( labels.docs , tolower )\r\nlabels.docs <- tm_map( labels.docs , removeWords , stopwords( \"english\" ) )\r\nlabels.docs <- tm_map( labels.docs , stripWhitespace )\r\nlabels.docs <- tm_map( labels.docs , PlainTextDocument ) \r\n\r\nlabels.docs <- tm_map(labels.docs, removeWords, c( \"name\" , \"twithndl\" , \"twithndlbanka\" , \"twithndlbankb\" , \"twithndlbankc\" , \"twithndlbankd\" , \"internet\" , \"rettwit\" ) )\r\n\r\nlabels.dtm  <- DocumentTermMatrix( labels.docs )\r\nlabels.freq <- colSums( as.matrix( labels.dtm ) )\r\n\r\nlabels.freq = find.freq.words( labels.scores , \"positive\" )\r\n\r\nlabels.ord <- order( labels.freq )\r\n\r\nlabels.freq[ tail( labels.ord , 50 ) ]\r\nlabels.freq[ tail( labels.ord , 50 ) ]\r\nwordcloud( names( labels.freq ), labels.freq , min.freq = 50 )  \r\n```","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}